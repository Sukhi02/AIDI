{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbbd93c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\envs\\arima\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from click->nltk) (0.4.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from click->nltk) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from importlib-metadata->click->nltk) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\envs\\arima\\lib\\site-packages (from importlib-metadata->click->nltk) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\hp\\ENVS\\ARIMA\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c28749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Preprocess Text Data\n",
    "> (Remove punctuation, Perform Tokenization, Remove stopwords and Lemmatize/Stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d87fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "0   I thought this was a wonderful way to spend ti...  positive\n",
       "1   Probably my all-time favorite movie, a story o...  positive\n",
       "2   I sure would like to see a resurrection of a u...  positive\n",
       "8   This a fantastic movie of three prisoners who ...  positive\n",
       "11  What an absolutely stunning movie, if you have...  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", sep=',')\n",
    "positive_records = data.loc[data[\"sentiment\"] == \"positive\"][:3000]\n",
    "negative_records = data.loc[data[\"sentiment\"] == \"negative\"][:3000]\n",
    "data = pd.concat([positive_records, negative_records], axis=0)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c620c",
   "metadata": {},
   "source": [
    "### 1. Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38f5ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What an absolutely stunning movie if you have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment  \\\n",
       "0   I thought this was a wonderful way to spend ti...  positive   \n",
       "1   Probably my all-time favorite movie, a story o...  positive   \n",
       "2   I sure would like to see a resurrection of a u...  positive   \n",
       "8   This a fantastic movie of three prisoners who ...  positive   \n",
       "11  What an absolutely stunning movie, if you have...  positive   \n",
       "\n",
       "                                         review_clean  \n",
       "0   I thought this was a wonderful way to spend ti...  \n",
       "1   Probably my alltime favorite movie a story of ...  \n",
       "2   I sure would like to see a resurrection of a u...  \n",
       "8   This a fantastic movie of three prisoners who ...  \n",
       "11  What an absolutely stunning movie if you have ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['review_clean'] = data['review'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b2c9e",
   "metadata": {},
   "source": [
    "### 2.Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43561e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>review_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of ...</td>\n",
       "      <td>[probably, my, alltime, favorite, movie, a, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>[this, a, fantastic, movie, of, three, prisone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What an absolutely stunning movie if you have ...</td>\n",
       "      <td>[what, an, absolutely, stunning, movie, if, yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment  \\\n",
       "0   I thought this was a wonderful way to spend ti...  positive   \n",
       "1   Probably my all-time favorite movie, a story o...  positive   \n",
       "2   I sure would like to see a resurrection of a u...  positive   \n",
       "8   This a fantastic movie of three prisoners who ...  positive   \n",
       "11  What an absolutely stunning movie, if you have...  positive   \n",
       "\n",
       "                                         review_clean  \\\n",
       "0   I thought this was a wonderful way to spend ti...   \n",
       "1   Probably my alltime favorite movie a story of ...   \n",
       "2   I sure would like to see a resurrection of a u...   \n",
       "8   This a fantastic movie of three prisoners who ...   \n",
       "11  What an absolutely stunning movie if you have ...   \n",
       "\n",
       "                                     review_tokenized  \n",
       "0   [i, thought, this, was, a, wonderful, way, to,...  \n",
       "1   [probably, my, alltime, favorite, movie, a, st...  \n",
       "2   [i, sure, would, like, to, see, a, resurrectio...  \n",
       "8   [this, a, fantastic, movie, of, three, prisone...  \n",
       "11  [what, an, absolutely, stunning, movie, if, yo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['review_tokenized'] = data['review_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9c9e8",
   "metadata": {},
   "source": [
    "### 3.Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b67d4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a8ae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>review_tokenized</th>\n",
       "      <th>review_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of ...</td>\n",
       "      <td>[probably, my, alltime, favorite, movie, a, st...</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrectio...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>[this, a, fantastic, movie, of, three, prisone...</td>\n",
       "      <td>[fantastic, movie, three, prisoners, become, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What an absolutely stunning movie if you have ...</td>\n",
       "      <td>[what, an, absolutely, stunning, movie, if, yo...</td>\n",
       "      <td>[absolutely, stunning, movie, 25, hrs, kill, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment  \\\n",
       "0   I thought this was a wonderful way to spend ti...  positive   \n",
       "1   Probably my all-time favorite movie, a story o...  positive   \n",
       "2   I sure would like to see a resurrection of a u...  positive   \n",
       "8   This a fantastic movie of three prisoners who ...  positive   \n",
       "11  What an absolutely stunning movie, if you have...  positive   \n",
       "\n",
       "                                         review_clean  \\\n",
       "0   I thought this was a wonderful way to spend ti...   \n",
       "1   Probably my alltime favorite movie a story of ...   \n",
       "2   I sure would like to see a resurrection of a u...   \n",
       "8   This a fantastic movie of three prisoners who ...   \n",
       "11  What an absolutely stunning movie if you have ...   \n",
       "\n",
       "                                     review_tokenized  \\\n",
       "0   [i, thought, this, was, a, wonderful, way, to,...   \n",
       "1   [probably, my, alltime, favorite, movie, a, st...   \n",
       "2   [i, sure, would, like, to, see, a, resurrectio...   \n",
       "8   [this, a, fantastic, movie, of, three, prisone...   \n",
       "11  [what, an, absolutely, stunning, movie, if, yo...   \n",
       "\n",
       "                                        review_nostop  \n",
       "0   [thought, wonderful, way, spend, time, hot, su...  \n",
       "1   [probably, alltime, favorite, movie, story, se...  \n",
       "2   [sure, would, like, see, resurrection, dated, ...  \n",
       "8   [fantastic, movie, three, prisoners, become, f...  \n",
       "11  [absolutely, stunning, movie, 25, hrs, kill, w...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data['review_nostop'] = data['review_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef87b45",
   "metadata": {},
   "source": [
    "### 4.Stem text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad73a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>review_tokenized</th>\n",
       "      <th>review_nostop</th>\n",
       "      <th>review_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of ...</td>\n",
       "      <td>[probably, my, alltime, favorite, movie, a, st...</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "      <td>[probabl, alltim, favorit, movi, stori, selfle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrectio...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>[sure, would, like, see, resurrect, date, seah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>[this, a, fantastic, movie, of, three, prisone...</td>\n",
       "      <td>[fantastic, movie, three, prisoners, become, f...</td>\n",
       "      <td>[fantast, movi, three, prison, becom, famou, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What an absolutely stunning movie if you have ...</td>\n",
       "      <td>[what, an, absolutely, stunning, movie, if, yo...</td>\n",
       "      <td>[absolutely, stunning, movie, 25, hrs, kill, w...</td>\n",
       "      <td>[absolut, stun, movi, 25, hr, kill, watch, won...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment  \\\n",
       "0   I thought this was a wonderful way to spend ti...  positive   \n",
       "1   Probably my all-time favorite movie, a story o...  positive   \n",
       "2   I sure would like to see a resurrection of a u...  positive   \n",
       "8   This a fantastic movie of three prisoners who ...  positive   \n",
       "11  What an absolutely stunning movie, if you have...  positive   \n",
       "\n",
       "                                         review_clean  \\\n",
       "0   I thought this was a wonderful way to spend ti...   \n",
       "1   Probably my alltime favorite movie a story of ...   \n",
       "2   I sure would like to see a resurrection of a u...   \n",
       "8   This a fantastic movie of three prisoners who ...   \n",
       "11  What an absolutely stunning movie if you have ...   \n",
       "\n",
       "                                     review_tokenized  \\\n",
       "0   [i, thought, this, was, a, wonderful, way, to,...   \n",
       "1   [probably, my, alltime, favorite, movie, a, st...   \n",
       "2   [i, sure, would, like, to, see, a, resurrectio...   \n",
       "8   [this, a, fantastic, movie, of, three, prisone...   \n",
       "11  [what, an, absolutely, stunning, movie, if, yo...   \n",
       "\n",
       "                                        review_nostop  \\\n",
       "0   [thought, wonderful, way, spend, time, hot, su...   \n",
       "1   [probably, alltime, favorite, movie, story, se...   \n",
       "2   [sure, would, like, see, resurrection, dated, ...   \n",
       "8   [fantastic, movie, three, prisoners, become, f...   \n",
       "11  [absolutely, stunning, movie, 25, hrs, kill, w...   \n",
       "\n",
       "                                       review_stemmed  \n",
       "0   [thought, wonder, way, spend, time, hot, summe...  \n",
       "1   [probabl, alltim, favorit, movi, stori, selfle...  \n",
       "2   [sure, would, like, see, resurrect, date, seah...  \n",
       "8   [fantast, movi, three, prison, becom, famou, o...  \n",
       "11  [absolut, stun, movi, 25, hr, kill, watch, won...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "data['review_nostop'] = data['review'].apply(lambda x: clean_text(x.lower()))\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['review_stemmed'] = data['review_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc09560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>review_tokenized</th>\n",
       "      <th>review_nostop</th>\n",
       "      <th>review_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Probably my alltime favorite movie a story of ...</td>\n",
       "      <td>[probably, my, alltime, favorite, movie, a, st...</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "      <td>[probabl, alltim, favorit, movi, stori, selfle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrectio...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>[sure, would, like, see, resurrect, date, seah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>[this, a, fantastic, movie, of, three, prisone...</td>\n",
       "      <td>[fantastic, movie, three, prisoners, become, f...</td>\n",
       "      <td>[fantast, movi, three, prison, becom, famou, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What an absolutely stunning movie if you have ...</td>\n",
       "      <td>[what, an, absolutely, stunning, movie, if, yo...</td>\n",
       "      <td>[absolutely, stunning, movie, 25, hrs, kill, w...</td>\n",
       "      <td>[absolut, stun, movi, 25, hr, kill, watch, won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Karen Carpenter Story shows a little more ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>The Karen Carpenter Story shows a little more ...</td>\n",
       "      <td>[the, karen, carpenter, story, shows, a, littl...</td>\n",
       "      <td>[karen, carpenter, story, shows, little, singe...</td>\n",
       "      <td>[karen, carpent, stori, show, littl, singer, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Taut and organically gripping, Edward Dmytryk'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Taut and organically gripping Edward Dmytryks ...</td>\n",
       "      <td>[taut, and, organically, gripping, edward, dmy...</td>\n",
       "      <td>[taut, organically, gripping, edward, dmytryks...</td>\n",
       "      <td>[taut, organ, grip, edward, dmytryk, crossfir,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"Ardh Satya\" is one of the finest film ever ma...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Ardh Satya is one of the finest film ever made...</td>\n",
       "      <td>[ardh, satya, is, one, of, the, finest, film, ...</td>\n",
       "      <td>[ardh, satya, one, finest, film, ever, made, i...</td>\n",
       "      <td>[ardh, satya, one, finest, film, ever, made, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>One of the most significant quotes from the en...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the most significant quotes from the en...</td>\n",
       "      <td>[one, of, the, most, significant, quotes, from...</td>\n",
       "      <td>[one, significant, quotes, entire, film, prono...</td>\n",
       "      <td>[one, signific, quot, entir, film, pronounc, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>This movie is based on the book, \"A Many Splen...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This movie is based on the book A Many Splendo...</td>\n",
       "      <td>[this, movie, is, based, on, the, book, a, man...</td>\n",
       "      <td>[movie, based, book, many, splendored, thing, ...</td>\n",
       "      <td>[movi, base, book, mani, splendor, thing, han,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment  \\\n",
       "0   I thought this was a wonderful way to spend ti...  positive   \n",
       "1   Probably my all-time favorite movie, a story o...  positive   \n",
       "2   I sure would like to see a resurrection of a u...  positive   \n",
       "8   This a fantastic movie of three prisoners who ...  positive   \n",
       "11  What an absolutely stunning movie, if you have...  positive   \n",
       "13  The Karen Carpenter Story shows a little more ...  positive   \n",
       "16  Taut and organically gripping, Edward Dmytryk'...  positive   \n",
       "17  \"Ardh Satya\" is one of the finest film ever ma...  positive   \n",
       "18  One of the most significant quotes from the en...  positive   \n",
       "24  This movie is based on the book, \"A Many Splen...  positive   \n",
       "\n",
       "                                         review_clean  \\\n",
       "0   I thought this was a wonderful way to spend ti...   \n",
       "1   Probably my alltime favorite movie a story of ...   \n",
       "2   I sure would like to see a resurrection of a u...   \n",
       "8   This a fantastic movie of three prisoners who ...   \n",
       "11  What an absolutely stunning movie if you have ...   \n",
       "13  The Karen Carpenter Story shows a little more ...   \n",
       "16  Taut and organically gripping Edward Dmytryks ...   \n",
       "17  Ardh Satya is one of the finest film ever made...   \n",
       "18  One of the most significant quotes from the en...   \n",
       "24  This movie is based on the book A Many Splendo...   \n",
       "\n",
       "                                     review_tokenized  \\\n",
       "0   [i, thought, this, was, a, wonderful, way, to,...   \n",
       "1   [probably, my, alltime, favorite, movie, a, st...   \n",
       "2   [i, sure, would, like, to, see, a, resurrectio...   \n",
       "8   [this, a, fantastic, movie, of, three, prisone...   \n",
       "11  [what, an, absolutely, stunning, movie, if, yo...   \n",
       "13  [the, karen, carpenter, story, shows, a, littl...   \n",
       "16  [taut, and, organically, gripping, edward, dmy...   \n",
       "17  [ardh, satya, is, one, of, the, finest, film, ...   \n",
       "18  [one, of, the, most, significant, quotes, from...   \n",
       "24  [this, movie, is, based, on, the, book, a, man...   \n",
       "\n",
       "                                        review_nostop  \\\n",
       "0   [thought, wonderful, way, spend, time, hot, su...   \n",
       "1   [probably, alltime, favorite, movie, story, se...   \n",
       "2   [sure, would, like, see, resurrection, dated, ...   \n",
       "8   [fantastic, movie, three, prisoners, become, f...   \n",
       "11  [absolutely, stunning, movie, 25, hrs, kill, w...   \n",
       "13  [karen, carpenter, story, shows, little, singe...   \n",
       "16  [taut, organically, gripping, edward, dmytryks...   \n",
       "17  [ardh, satya, one, finest, film, ever, made, i...   \n",
       "18  [one, significant, quotes, entire, film, prono...   \n",
       "24  [movie, based, book, many, splendored, thing, ...   \n",
       "\n",
       "                                       review_stemmed  \n",
       "0   [thought, wonder, way, spend, time, hot, summe...  \n",
       "1   [probabl, alltim, favorit, movi, stori, selfle...  \n",
       "2   [sure, would, like, see, resurrect, date, seah...  \n",
       "8   [fantast, movi, three, prison, becom, famou, o...  \n",
       "11  [absolut, stun, movi, 25, hr, kill, watch, won...  \n",
       "13  [karen, carpent, stori, show, littl, singer, k...  \n",
       "16  [taut, organ, grip, edward, dmytryk, crossfir,...  \n",
       "17  [ardh, satya, one, finest, film, ever, made, i...  \n",
       "18  [one, signific, quot, entir, film, pronounc, h...  \n",
       "24  [movi, base, book, mani, splendor, thing, han,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "def clean_text_tfidf(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c97a6",
   "metadata": {},
   "source": [
    "## Q2: TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a6da3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 61307)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text_tfidf)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['review'])\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85604739",
   "metadata": {},
   "source": [
    "## Q3: Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1dbac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ---- Precision: 0.678 / Recall: 0.746 / Accuracy: 0.704\n",
      "Est: 10 / Depth: 20 ---- Precision: 0.682 / Recall: 0.707 / Accuracy: 0.698\n",
      "Est: 10 / Depth: 30 ---- Precision: 0.72 / Recall: 0.732 / Accuracy: 0.732\n",
      "Est: 10 / Depth: None ---- Precision: 0.76 / Recall: 0.635 / Accuracy: 0.725\n",
      "Est: 50 / Depth: 10 ---- Precision: 0.782 / Recall: 0.828 / Accuracy: 0.804\n",
      "Est: 50 / Depth: 20 ---- Precision: 0.805 / Recall: 0.83 / Accuracy: 0.82\n",
      "Est: 50 / Depth: 30 ---- Precision: 0.827 / Recall: 0.842 / Accuracy: 0.838\n",
      "Est: 50 / Depth: None ---- Precision: 0.827 / Recall: 0.804 / Accuracy: 0.823\n",
      "Est: 100 / Depth: 10 ---- Precision: 0.805 / Recall: 0.851 / Accuracy: 0.828\n",
      "Est: 100 / Depth: 20 ---- Precision: 0.831 / Recall: 0.852 / Accuracy: 0.844\n",
      "Est: 100 / Depth: 30 ---- Precision: 0.829 / Recall: 0.856 / Accuracy: 0.844\n",
      "Est: 100 / Depth: None ---- Precision: 0.852 / Recall: 0.832 / Accuracy: 0.848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y=data['sentiment'].map({'positive':1,'negative':0})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.2)\n",
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))\n",
    "    \n",
    "#               ***************REPLACE IT WITH GRIDSEARCH CV METHOD********************\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# GridSearchCV(estimator=SVC(),param_grid=[10, 50])\n",
    "for n_est in [10, 50,100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "         train_RF(n_est, depth)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4285d719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 50 / Depth: 3 / LR: 0.01 ---- Precision: 0.631 / Recall: 0.916 / Accuracy: 0.694\n",
      "Est: 50 / Depth: 3 / LR: 0.1 ---- Precision: 0.714 / Recall: 0.863 / Accuracy: 0.762\n",
      "Est: 50 / Depth: 3 / LR: 1 ---- Precision: 0.756 / Recall: 0.821 / Accuracy: 0.781\n",
      "Est: 50 / Depth: 7 / LR: 0.01 ---- Precision: 0.653 / Recall: 0.883 / Accuracy: 0.711\n",
      "Est: 50 / Depth: 7 / LR: 0.1 ---- Precision: 0.75 / Recall: 0.84 / Accuracy: 0.782\n",
      "Est: 50 / Depth: 7 / LR: 1 ---- Precision: 0.724 / Recall: 0.826 / Accuracy: 0.759\n",
      "Est: 50 / Depth: 11 / LR: 0.01 ---- Precision: 0.669 / Recall: 0.843 / Accuracy: 0.717\n",
      "Est: 50 / Depth: 11 / LR: 0.1 ---- Precision: 0.759 / Recall: 0.845 / Accuracy: 0.791\n",
      "Est: 50 / Depth: 11 / LR: 1 ---- Precision: 0.73 / Recall: 0.826 / Accuracy: 0.763\n",
      "Est: 50 / Depth: 15 / LR: 0.01 ---- Precision: 0.671 / Recall: 0.838 / Accuracy: 0.718\n",
      "Est: 50 / Depth: 15 / LR: 0.1 ---- Precision: 0.754 / Recall: 0.861 / Accuracy: 0.793\n",
      "Est: 50 / Depth: 15 / LR: 1 ---- Precision: 0.725 / Recall: 0.848 / Accuracy: 0.767\n",
      "Est: 100 / Depth: 3 / LR: 0.01 ---- Precision: 0.653 / Recall: 0.904 / Accuracy: 0.716\n",
      "Est: 100 / Depth: 3 / LR: 0.1 ---- Precision: 0.757 / Recall: 0.877 / Accuracy: 0.8\n",
      "Est: 100 / Depth: 3 / LR: 1 ---- Precision: 0.747 / Recall: 0.838 / Accuracy: 0.78\n",
      "Est: 100 / Depth: 7 / LR: 0.01 ---- Precision: 0.678 / Recall: 0.867 / Accuracy: 0.731\n",
      "Est: 100 / Depth: 7 / LR: 0.1 ---- Precision: 0.768 / Recall: 0.856 / Accuracy: 0.802\n",
      "Est: 100 / Depth: 7 / LR: 1 ---- Precision: 0.748 / Recall: 0.836 / Accuracy: 0.78\n",
      "Est: 100 / Depth: 11 / LR: 0.01 ---- Precision: 0.684 / Recall: 0.858 / Accuracy: 0.734\n",
      "Est: 100 / Depth: 11 / LR: 0.1 ---- Precision: 0.762 / Recall: 0.861 / Accuracy: 0.799\n",
      "Est: 100 / Depth: 11 / LR: 1 ---- Precision: 0.754 / Recall: 0.851 / Accuracy: 0.79\n",
      "Est: 100 / Depth: 15 / LR: 0.01 ---- Precision: 0.689 / Recall: 0.848 / Accuracy: 0.736\n",
      "Est: 100 / Depth: 15 / LR: 0.1 ---- Precision: 0.769 / Recall: 0.858 / Accuracy: 0.802\n",
      "Est: 100 / Depth: 15 / LR: 1 ---- Precision: 0.75 / Recall: 0.868 / Accuracy: 0.792\n",
      "Est: 150 / Depth: 3 / LR: 0.01 ---- Precision: 0.657 / Recall: 0.902 / Accuracy: 0.719\n",
      "Est: 150 / Depth: 3 / LR: 0.1 ---- Precision: 0.764 / Recall: 0.883 / Accuracy: 0.808\n",
      "Est: 150 / Depth: 3 / LR: 1 ---- Precision: 0.754 / Recall: 0.848 / Accuracy: 0.788\n",
      "Est: 150 / Depth: 7 / LR: 0.01 ---- Precision: 0.691 / Recall: 0.868 / Accuracy: 0.743\n",
      "Est: 150 / Depth: 7 / LR: 0.1 ---- Precision: 0.779 / Recall: 0.863 / Accuracy: 0.812\n",
      "Est: 150 / Depth: 7 / LR: 1 ---- Precision: 0.761 / Recall: 0.845 / Accuracy: 0.792\n",
      "Est: 150 / Depth: 11 / LR: 0.01 ---- Precision: 0.7 / Recall: 0.865 / Accuracy: 0.751\n",
      "Est: 150 / Depth: 11 / LR: 0.1 ---- Precision: 0.775 / Recall: 0.865 / Accuracy: 0.809\n",
      "Est: 150 / Depth: 11 / LR: 1 ---- Precision: 0.757 / Recall: 0.856 / Accuracy: 0.793\n",
      "Est: 150 / Depth: 15 / LR: 0.01 ---- Precision: 0.713 / Recall: 0.863 / Accuracy: 0.761\n",
      "Est: 150 / Depth: 15 / LR: 0.1 ---- Precision: 0.777 / Recall: 0.858 / Accuracy: 0.808\n",
      "Est: 150 / Depth: 15 / LR: 1 ---- Precision: 0.764 / Recall: 0.856 / Accuracy: 0.798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.2)\n",
    "\n",
    "def train_GB(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, average='binary')\n",
    "    print('Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        est, max_depth, lr, round(precision, 3), round(recall, 3), \n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)))   \n",
    "    \n",
    "for n_est in [50, 100,150]:\n",
    "    for max_depth in [3, 7,11,15]:\n",
    "        for lr in [0.01,0.1, 1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Perform Final evaluation of models on the best parameter settings using the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b289851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.450892</td>\n",
       "      <td>0.838965</td>\n",
       "      <td>0.604457</td>\n",
       "      <td>0.032535</td>\n",
       "      <td>60</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 300}</td>\n",
       "      <td>0.831667</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.848333</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.847333</td>\n",
       "      <td>0.009936</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68.441679</td>\n",
       "      <td>0.345525</td>\n",
       "      <td>0.579685</td>\n",
       "      <td>0.023108</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.835833</td>\n",
       "      <td>0.840833</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.859167</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>58.649823</td>\n",
       "      <td>2.181192</td>\n",
       "      <td>0.367862</td>\n",
       "      <td>0.072871</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.860833</td>\n",
       "      <td>0.834167</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.843833</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35.971511</td>\n",
       "      <td>1.322008</td>\n",
       "      <td>0.283747</td>\n",
       "      <td>0.022659</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.855833</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.842833</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.709712</td>\n",
       "      <td>0.403648</td>\n",
       "      <td>0.481467</td>\n",
       "      <td>0.015017</td>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 300}</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.838333</td>\n",
       "      <td>0.851667</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5       57.450892      0.838965         0.604457        0.032535   \n",
       "8       68.441679      0.345525         0.579685        0.023108   \n",
       "11      58.649823      2.181192         0.367862        0.072871   \n",
       "10      35.971511      1.322008         0.283747        0.022659   \n",
       "2       31.709712      0.403648         0.481467        0.015017   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "5               60                300   \n",
       "8               90                300   \n",
       "11            None                300   \n",
       "10            None                150   \n",
       "2               30                300   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "5     {'max_depth': 60, 'n_estimators': 300}           0.831667   \n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.835833   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.826667   \n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.826667   \n",
       "2     {'max_depth': 30, 'n_estimators': 300}           0.830833   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "5            0.844167           0.848333           0.850000   \n",
       "8            0.840833           0.852500           0.836667   \n",
       "11           0.844167           0.860833           0.834167   \n",
       "10           0.837500           0.855833           0.832500   \n",
       "2            0.837500           0.854167           0.838333   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "5            0.862500         0.847333        0.009936                1  \n",
       "8            0.859167         0.845000        0.009250                2  \n",
       "11           0.853333         0.843833        0.012390                3  \n",
       "10           0.861667         0.842833        0.013567                4  \n",
       "2            0.851667         0.842500        0.008929                5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_tfidf,y)\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. GradientBoostingClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f623ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244.765280</td>\n",
       "      <td>2.060118</td>\n",
       "      <td>0.035634</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.815833</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.025486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394.437976</td>\n",
       "      <td>1.582940</td>\n",
       "      <td>0.049506</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.807833</td>\n",
       "      <td>0.020887</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167.157986</td>\n",
       "      <td>1.881748</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.800833</td>\n",
       "      <td>0.822500</td>\n",
       "      <td>0.798333</td>\n",
       "      <td>0.828333</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.023184</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>422.735294</td>\n",
       "      <td>13.814425</td>\n",
       "      <td>0.029606</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.802333</td>\n",
       "      <td>0.025289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268.539047</td>\n",
       "      <td>3.734401</td>\n",
       "      <td>0.039477</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.765833</td>\n",
       "      <td>0.799167</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.799167</td>\n",
       "      <td>0.825833</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1     244.765280      2.060118         0.035634        0.006181   \n",
       "3     394.437976      1.582940         0.049506        0.008605   \n",
       "0     167.157986      1.881748         0.028906        0.006685   \n",
       "5     422.735294     13.814425         0.029606        0.007483   \n",
       "2     268.539047      3.734401         0.039477        0.006920   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "1                 0.1               7                150   \n",
       "3                 0.1              11                150   \n",
       "0                 0.1               7                100   \n",
       "5                 0.1              15                150   \n",
       "2                 0.1              11                100   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "1  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.772500   \n",
       "3  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.770833   \n",
       "0  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.762500   \n",
       "5  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.766667   \n",
       "2  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.765833   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "1           0.805833           0.837500           0.815833           0.844167   \n",
       "3           0.802500           0.823333           0.811667           0.830833   \n",
       "0           0.800833           0.822500           0.798333           0.828333   \n",
       "5           0.785000           0.823333           0.800000           0.836667   \n",
       "2           0.799167           0.820000           0.799167           0.825833   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "1         0.815167        0.025486                1  \n",
       "3         0.807833        0.020887                2  \n",
       "0         0.802500        0.023184                3  \n",
       "5         0.802333        0.025289                4  \n",
       "2         0.802000        0.021053                5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "cv_fit = clf.fit(X_tfidf,y)\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fde79",
   "metadata": {},
   "source": [
    "## Q5: Report the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the experiment, the Random Forest has \n",
    "GradientBoostingClassifier is the best model. \n",
    "\n",
    "> Random Forest  = 0.304306 (mean_score_time) \t0.098282 \t(std_score_time) <br>\n",
    "> GradientBoostingClassifier = 0.020673 (mean_score_time)\t0.002118 (std_score_time)\t\n",
    "\n",
    "<b>Thus, The Gradient Boster </b> has the least score time, so the best peroforming model is it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
